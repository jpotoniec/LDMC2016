\documentclass{llncs}
\usepackage{url}

\begin{document}

\title{Not-So-Linked Solution to the Linked Data Mining Challenge 2016}
\author{Jedrzej Potoniec}
\institute{Institute of Computing Science, Poznan University of Technology\\ul. Piotrowo 2, 60-965 Poznan, Poland\\
\email{Jedrzej.Potoniec@cs.put.poznan.pl}}

\maketitle

\begin{abstract}
We present a solution for the \emph{Linked Data Mining Challenge 2016}, that achieved $92.5\%$ accuracy according to the submission system.
The solution uses a hand-crafted dataset, that was created by scraping various websites for reviews.
We use logistic regression to learn a classification model and we publish all our results to \emph{GitHub}.
\end{abstract}


\section{Introduction}

As indicated in the challenge website, Linked Data Mining is a novel and challenging research area, mainly due to large amount, variety and heterogeneity of the data.
Unfortunately, there are also very basic, almost technical, problems with the data: they do not comply with the standards, there is a lot of mistakes introduced during extraction and transformation from an original format to the Linked Data, websites publishing the data are frequently down \cite{lodlaudromat}.
Because of that, we decided to take another path in our solution.
We did the extraction by ourselves, thus receiving dataset well-suited for the challenge, as described in Section \ref{sec:data}.
We performed normalization and applied a very popular logistic regression method to train a classification model, as described in Section \ref{sec:ml}.

Throughout the rest of the paper, we use a prefix \texttt{dbr:} for \url{http://dbpedia.org/resource/} and \texttt{dbp:} for \url{http://dbpedia.org/property/}.
Web scraping scripts, created dataset, machine learning process and model are available on \emph{GitHub}: \url{https://github.com/jpotoniec/LDMC2016}.

\section{Datasets\label{sec:data}}
\subsection{Training and test data}
We observed some irregularities and unexpected things in the datasets provided by the challenge.
For the album \emph{In Some Way, Shape, or Form} by \emph{Four Year Strong} the data pointed to the resource \url{dbr:In_Some_Way,_Shape_or_Form}.
Unfortunately, in the \emph{DBpedia} \cite{dbpedia} there are two corresponding resources, the other one being \url{dbr:In_Some_Way,_Shape,_or_Form} (note the second comma).
The \emph{DBpedia} website does some sort of redirection, so when visiting with a web browser both URIs point to \url{http://dbpedia.org/page/In_Some_Way,_Shape,_or_Form}.
Conversely, the SPARQL endpoint\footnote{\url{http://dbpedia.org/sparql}} treats both URIs separately, the first one occurring in 18 triples and the second one in 100 triples.

For an artist \emph{St. Vincent} there are two albums in the datasets: \emph{Strange Mercy} in the training data and \emph{St. Vincent} in the testing data.
Unfortunately, both have the same URI \url{dbr:Strange_Mercy}.
We think there may be a few similar issues, as there are eight URIs that occur more than once in the datasets.
%
%There was also a minor issue with the description of the datasets.
%The challenge website claims that \emph{music albums with score above 79 are regarded as "good" music albums, while music albums with score less than 40 are regarded as "bad" music albums}.
%Apprantly, only the first part of the sentence is true and everything below 79 was classified as "bad".

\subsection{Linked datasets\label{sec:ld}}
In the beginning, we planned to extract features from \emph{DBpedia} using \emph{Fr-ONT-Qu} \cite{frontqu} from \emph{RMonto} \cite{rmonto}, a plugin to \emph{RapidMiner} \cite{rapidminer}.
Unfortunately, the most promising feature discovered this way was a binary information if an album has a review score from \emph{Pitchfork}\footnote{\url{http://pitchfork.com/}} or not.
After investigating, we discovered that during the extraction from \emph{Wikipedia} to \emph{DBpedia} a relation between a reviewing website and a review score has been lost.
Consider triples for the \emph{Strange Mercy} album\footnote{\url{http://dbpedia.org/page/Strange_Mercy}}: there are 11 triples with a property \texttt{dbp:rev} and a few triples with properties like \texttt{dbp:rev10score}, but one has absolutely no way to connect scores to the reviewing websites.
The very same problem happens with properties \texttt{dbp:title} (12 values) and \texttt{dbp:length} (11 values): it is impossible to decide on a length for a concrete track.
Due to the lack of space, we present a detailed analysis in the suplementary material available in \emph{GitHub}.

We thought about using \emph{Yago} \cite{yago}, but it seemed to lack review information.
We also tried to use \emph{DBTune}\footnote{\url{http://dbtune.org/}}, as suggested by the challenge website, but it rendered out to be a dead end.
For example, \emph{MusicBrainz data}, the most interesting dataset there, is a \emph{Service Temporarily Unavailable} for a very long time now.

\subsection{Non-linked datasets\label{sec:nonld}}
Instead of trying to use existing Linked Data, we decided find data to solve the challenge, and then make it available to the community.
As the datasets for the challenge are quite small (1592 different URIs), we did some web scrapping with \emph{Python} and \emph{Scrapy}\footnote{\url{http://scrapy.org/}} to obtain reviews of considered albums.

We scraped \emph{Wikipedia} to obtain reviews collected from various websites.
It rendered out to be a tedious process, as these reviews have various formats, frequently with some additional information (like a date or an URL to a review), or spelling mistakes.
We performed normalization to a range $\left[0,1\right]$, by dividing in case of reviews on scales from 0 to 10 or 100 and by assigning arbitrarily numeric values to descriptive reviews (like \emph{favorable}).
We also did some heuristic to normalize reviewing websites, e.g. \emph{BBC} and \emph{BBC Music}.
We strictly avoided using \emph{Metacritic} reviews available in \emph{Wikipedia}: these reviews use \texttt{MC} field in \texttt{Album ratings} template, while we used only fields starting with \texttt{rev} \cite{album_ratings}.

We collected number of reviewers and an average score from \emph{Amazon}\footnote{\url{http://www.amazon.com/}} by scraping the website using titles and artists provided in the challenge datasets.
We also used \emph{Discogs}\footnote{\url{https://www.discogs.com/}} API and provided titles and artists to gather how many people own an album and how many people want it.
Finally, we used datadumps provided by \emph{MusicBrainz}\footnote{\url{https://musicbrainz.org/doc/MusicBrainz_Database/Download}} \cite{musicbrainz} and identifiers from Wikidata \cite{wikidata} available in the datadumps and in \emph{DBpedia}, to obtain number of users owning an album and it's average score.
The whole dataset consists of 94 numerical attributes.

\section{Machine learning process and model\label{sec:ml}}

To build a classification model, we used a typical machine learning setup for classification.
We performed a Z-transformation on all attributes, that is for every attribute we computed an average value $\mu$ and a standard deviation $\sigma$, and then replaced every value $v$ of the attribute with $\frac{v-\mu}{\sigma}$.
This way all attributes have an average 0 and a standard deviation 1.
Further, we replaced missing values with 0.
Finally, we used logistic regression \cite{logistic_regression} to train the model.

To estimate performance of our solution we applied 10-folds cross-validation, which estimated accuracy to be $91.7\pm 2.17\%$.
This value is consistent with $92.5\%$ on the test set reported by the challenge submission system.
The whole process have been implemented using \emph{RapidMiner 5} and is available in \emph{GitHub}.

An important part of logistic regression is to assign coefficients to attributes of an input dataset.
Values of these coefficients provide an insight which attributes are most important for the model.
In our case, the absolute value of the highest coefficient is $2.859$ and the lowest $0.022$.
As all the attributes are on the same scale, this clearly shows that some of them are more important than the others.
There were six attributes having coefficients above 1, we present them in the Table \ref{tab:best_attrs}.
Five of these attributes were review scores web scrapped from \emph{Wikipedia}, only the attribute from \emph{Discogs} came from other source.
These attributes clearly indicate that \emph{Metacritic} reviews are quite consistent with other sources of reviews.
The attribute with the highest coefficient is an review value from \emph{Pitchfork}, what is consistent with the most important attribute from \emph{Linked Data}, as mentioned in Section \ref{sec:ld}.
The attribute from \emph{Discogs} indicates how many people own an album and is probably a tendency of people to buy and brag about albums that have good reviews.
The attribute with the lowest weight $-0.442$ is a number of reviews of an album on \emph{Amazon}.
As \emph{Amazon} is a shop, it probably shows a tendency of people to complain about bad things and to not appreciate good things.

\begin{table}
\caption{The attributes having coefficients in logistic regression model above 1.
These coefficients were all positive, what means that the higher they are, the higher probability of a given album being a good one.
\label{tab:best_attrs}
}
\begin{tabular}{l|r}
attribute & coefficient \\
\hline
review score from \emph{Pitchfork} \url{pitchfork.com} & $2.859$ \\
review score from \emph{AllMusic} \url{www.allmusic.com} & $2.437$ \\
review score from \emph{Stylus} \url{www.stylusmagazine.com} & $1.926$ \\ %TODO: sprawdzic czy to to
number of people owning an album according to \emph{Discogs} \url{www.discogs.com} & $1.465$ \\
review score from \emph{Entertainment Weekly} \url{www.ew.com} & $1.274$ \\ %TODO: sprawdzic czy to to
review score from \emph{The Guardian} \url{www.theguardian.com} & $1.096$ \\ %TODO: sprawdzic czy to to
\end{tabular}
\end{table}

\section{Conclusions\label{sec:concl}}

Apparently, we are not there yet with the Semantic Web.
In theory, most of the features we used were already available in the Linked Data.
In practice, they were not.
The issues with the Linked Data discussed in the paper clearly suggests that even a very simple and crude solutions doing web scrapping can easily outperform solutions based on the Linked Data.

The presented solution consists of 641 lines of Python code and can classify correctly 296 out of 320 test albums, which we find to be quite a good result given a small amount of time invested and irregularities in the provided datasets.
It is also worth to note, that the baseline solution was able to classify correctly 222 test albums ($69.375\%$), so our solution offers quite an improvement.

\bibliographystyle{splncs03}
\bibliography{ldmc}

\end{document}
